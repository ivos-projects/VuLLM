# VuLLM - Vulnerability Language Model

This project will investige if the representations generated by pretraining a
large language model can be used to enhance vulnerability classification. This
project investigates two different approaches, a sequence model and a causal
model. The sequence model is a simple model that uses the pretrained language
with a linear layer with two logits as the head. The causal model uses the
original lm head and are steered towards the classification task by using a
prompt.

## Report
The report for this project can be found [here](./vullm.pdf).

## Datasets

**Reveal** contains 22,734 examples of code snippets, labels as vulnerable or
not vulnerable. The dataset is available on
[huggingface](https://huggingface.co/datasets/Oscaraandersson/reveal).

## Scripts
The repositry contains four main scripts. Two are for the sequence model and two
for the causal model. Each model has both a training script and an inference
script. They are found in ``src/sequence_model`` and ``src/causal_model``

The scripts take to arguments, ``--modelname`` and ``--config``. The modelname
is the key of the model in the config file. The config file is a json file
containing the hyperparameters for the model. The config file is found in
``config.json``.

Below is a table with explaining the different hyperparameters.

| Hyperparameter                 | Description                                                   | Data Type  |
| ------------------------------ | ------------------------------------------------------------- | ---------- |
| `basemodel`                    | The base model used for training.                             | String     |
| `adapter_path`                 | The path where the model adapter is stored.                   | String     |
| `logdir`                       | The directory where logs are stored.                          | String     |
| `dataset`                      | The dataset split. One of 'valid' and 'test'.                 | String     |
| `balanced`                     | A boolean indicating whether the dataset is balanced or not.  | Boolean    |
| `max_length`                   | The maximum length of the sequences for the model.             | Integer    |
| `max_samples`                  | The maximum number of samples to use from the dataset.         | Integer    |
| `truncated_train`              | A boolean indicating whether to truncate the training data or not. | Boolean |
| `prompt_template`              | A boolean indicating whether to use a prompt template or not. | Boolean    |
| `run_name`                     | The name of the run.                                          | String     |
| `per_device_train_batch_size`  | The batch size for training per device.                        | Integer    |
| `gradient_accumulation_steps`  | The number of steps to accumulate gradients before updating model parameters. | Integer |
| `warmup_steps`                 | The number of steps for the warmup phase, gradually increase the learning rate. | Integer |
| `epochs`                       | The number of epochs for training.                             | Integer    |
| `learning_rate`                | The learning rate for the optimizer.                           | Float      |
| `max_eval_samples`             | The maximum number of evaluation samples to use from the dataset. | Integer |
| `report_to`                    | The destination to report the results to. 'wandb' or 'none'   | String     |

## Models
To access the Llama models, access on huggingface are needed. find more on the 
huggingface [website](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).

## Installation
Start by creating a virtual environment and install the dependencies. 
Python 3.11.6 was used in this project

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

This project should be build as a local package. To do this, run the following
command from the root directory of the project.

```bash
pip install -e .
```
